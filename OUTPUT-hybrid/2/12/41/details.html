<html><h3>86ce5d52134a56806112ff8664e4034338e0e05a,yarll/agents/ppo/ppo.py,PPO,learn,#PPO#,208
</h3><img src='109570.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
                    batch_actions = np.array(actions)[batch_indices]
                    batch_advs = np.array(advs)[batch_indices]
                    batch_rs = np.array(rs)[batch_indices]
                    fetches = <a id="change">[self.train_op]</a>
                    if (n_updates % 1000) == 0:
                        fetches.append(self.model_summary_op)
                    feed_dict = <a id="change">{
                        self.states: batch_states,
                        self.old_network.states: batch_states,
                        self.actions_taken: batch_actions,
                        self.old_network.actions_taken: batch_actions,
                        self.advantage: batch_advs,
                        self.ret: batch_rs
                    }</a>
                    <a id="change">results = self.session.run(fetches, feed_dict)</a>
                    if (n_updates % 1000) == 0:
                        self.writer.add_summary(results[-1], n_updates)
                    n_updates += 1
                self.writer.flush()</code></pre><h3>After Change</h3><pre><code class='java'>
        Run learning algorithm
        config = self.config
        n_updates = 0
        <a id="change">with self.writer.as_default():
            for _ in range(int(config["n_iter"])):
                &#47&#47 Collect trajectories until we get timesteps_per_batch total timesteps
                states, actions, advs, rs, _ = self.get_processed_trajectories()
                advs = np.array(advs)
                advs = (advs - advs.mean()) / advs.std()
                self.set_old_to_new()

                indices = np.arange(len(states))
                for _ in range(int(self.config["n_epochs"])):
                    np.random.shuffle(indices)
                    batch_size = int(self.config["batch_size"])
                    for j in range(0, len(states), batch_size):
                        batch_indices = indices[j:(j + batch_size)]
                        batch_states = np.array(states)[batch_indices]
                        batch_actions = np.array(actions)[batch_indices]
                        batch_advs = np.array(advs)[batch_indices]
                        batch_rs = np.array(rs)[batch_indices]
                        train_actor_loss, train_critic_loss, train_loss = self.train(batch_states,
                                                                                     batch_actions,
                                                                                     batch_advs,
                                                                                     batch_rs)
                        tf.summary.scalar("model/loss", train_loss, step=n_updates)
                        tf.summary.scalar("model/actor_loss", train_actor_loss, step=n_updates)
                        tf.summary.scalar("model/critic_loss", train_critic_loss, step=n_updates)
                        n_updates += 1

            if self.config["save_model"]:
                tf.saved_model.save(self.ac_net, os.path.join(self.monitor_path, "model"))


</a>class PPODiscrete(PPO):
    def build_networks(self) -&gt; ActorCriticNetwork:
        return ActorCriticNetworkDiscrete(
            self.env.action_space.n,</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 11</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/arnomoonens/yarll/commit/86ce5d52134a56806112ff8664e4034338e0e05a#diff-b6ad861197e3ed50385bc08dbb9454faf358b1c1140c6a806c07997faf1551e6L189' target='_blank'>Link</a></div><div id='project'> Project Name: arnomoonens/yarll</div><div id='commit'> Commit Name: 86ce5d52134a56806112ff8664e4034338e0e05a</div><div id='time'> Time: </div><div id='author'> Author: null</div><div id='file'> File Name: yarll/agents/ppo/ppo.py</div><div id='class'> Class Name: PPO</div><div id='method'> Method Name: learn</div><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/641a28fbf0daff0ad1ad0f43d2c4b545cb6f9656#diff-1702490f9eb46e983be30c93f51fa12ca85f2bb222524f54c295abacb9691eb0L142' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: 641a28fbf0daff0ad1ad0f43d2c4b545cb6f9656</div><div id='time'> Time: </div><div id='author'> Author: null</div><div id='file'> File Name: examples/reinforcement_learning/tutorial_cartpole_ac.py</div><div id='class'> Class Name: Critic</div><div id='method'> Method Name: learn</div><BR><BR><div id='link'><a href='https://github.com/tensorlayer/tensorlayer/commit/641a28fbf0daff0ad1ad0f43d2c4b545cb6f9656#diff-1702490f9eb46e983be30c93f51fa12ca85f2bb222524f54c295abacb9691eb0L106' target='_blank'>Link</a></div><div id='project'> Project Name: tensorlayer/tensorlayer</div><div id='commit'> Commit Name: 641a28fbf0daff0ad1ad0f43d2c4b545cb6f9656</div><div id='time'> Time: </div><div id='author'> Author: null</div><div id='file'> File Name: examples/reinforcement_learning/tutorial_cartpole_ac.py</div><div id='class'> Class Name: Actor</div><div id='method'> Method Name: learn</div><BR>