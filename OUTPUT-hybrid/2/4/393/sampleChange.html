<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
      if True, restore the model from the most recent checkpoint and continue training
      from there.  If False, retrain the model from scratch.
    
    <a id="change">with self._graph.as_default():
      self._session.run(tf.global_variables_initializer())
      if restore:
        self.restore()
      manager = tf.train.CheckpointManager(self._checkpoint, self.model_dir,
                                           max_checkpoints_to_keep)
      checkpoint_time = time.time()

      &#47&#47 Main optimization loop.

      for i in range(steps):
        self._session.run(self._clear_gradients)
        for j in range(self.meta_batch_size):
          self.learner.select_task()
          feed_dict = self.learner.get_batch()
          feed_dict[self._global_step] = i
          for key, value in self.learner.get_batch().items():
            feed_dict[self._meta_placeholders[key]] = value
          self._session.run(self._add_gradients, feed_dict=feed_dict)
        self._session.run(self._meta_train_op)

        &#47&#47 Do checkpointing.

        if i == steps - 1 or time.time(
        ) &gt;= checkpoint_time + checkpoint_interval:
          with self._session.as_default():
            manager.save()
          checkpoint_time = time.time()

 </a> def restore(self):
    Reload the model parameters from the most recent checkpoint file.
    last_checkpoint = tf.train.latest_checkpoint(self.model_dir)
    if last_checkpoint is None:</code></pre><h3>After Change</h3><pre><code class='java'>
        feed_dict[self._global_step] = i
        for k in range(len(inputs)):
          feed_dict[self._input_placeholders[k]] = inputs[k]
          feed_dict[self._meta_placeholders[k]] = <a id="change">inputs[k]</a>
        self._session.run(self._add_gradients, feed_dict=feed_dict)
      self._session.run(self._meta_train_op)

      &#47&#47 Do checkpointing.</code></pre>