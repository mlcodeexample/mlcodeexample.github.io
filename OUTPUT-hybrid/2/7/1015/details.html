<html><h3>72a1d9d426004269997f8b40bdd64f8ee582d91e,rl_coach/agents/policy_optimization_agent.py,PolicyOptimizationAgent,train,#PolicyOptimizationAgent#,90
</h3><img src='1381652.png'><BR><BR><BR><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            &#47&#47 get t_max transitions or less if the we got to a terminal state
            &#47&#47 will be used for both actor-critic and vanilla PG.
            &#47&#47 &#47&#47 In order to get full episodes, Vanilla PG will set the end_idx to a very big value.
            <a id="change">transitions = []</a>
            start_idx = self.last_gradient_update_step_idx
            end_idx = episode.length()

            <a id="change">for idx in range(start_idx, end_idx):
                transitions.append(episode.get_transition(idx))
           </a> self.last_gradient_update_step_idx = end_idx

            &#47&#47 update the statistics for the variance reduction techniques
            if self.policy_gradient_rescaler in \</code></pre><h3>After Change</h3><pre><code class='java'>
            &#47&#47 get t_max transitions or less if the we got to a terminal state
            &#47&#47 will be used for both actor-critic and vanilla PG.
            &#47&#47 In order to get full episodes, Vanilla PG will set the end_idx to a very big value.
            transitions = <a id="change">episode[self.last_gradient_update_step_idx:]</a>
            batch = Batch(transitions)

            &#47&#47 move the pointer for the last update step
            if episode.is_complete:</code></pre><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 6</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/NervanaSystems/coach/commit/72a1d9d426004269997f8b40bdd64f8ee582d91e#diff-9be55ccde8ff567667fd1bae737d38e12549d0cf1a514bb0c43774ed3e83621bL85' target='_blank'>Link</a></div><div id='project'> Project Name: NervanaSystems/coach</div><div id='commit'> Commit Name: 72a1d9d426004269997f8b40bdd64f8ee582d91e</div><div id='time'> Time: </div><div id='author'> Author: null</div><div id='file'> File Name: rl_coach/agents/policy_optimization_agent.py</div><div id='class'> Class Name: PolicyOptimizationAgent</div><div id='method'> Method Name: train</div><BR><BR><div id='link'><a href='https://github.com/tensorflow/lattice/commit/1c75176947730de8322acf6ad996096625e92e3a#diff-9a3b998d3248524fa6a29ff883da259bac34265ff0749c17d3c708c952c34b8eL220' target='_blank'>Link</a></div><div id='project'> Project Name: tensorflow/lattice</div><div id='commit'> Commit Name: 1c75176947730de8322acf6ad996096625e92e3a</div><div id='time'> Time: </div><div id='author'> Author: null</div><div id='file'> File Name: tensorflow_lattice/python/rtl_layer.py</div><div id='class'> Class Name: RTL</div><div id='method'> Method Name: call</div><BR><BR><div id='link'><a href='https://github.com/NifTK/NiftyNet/commit/534c4dc7423681faab78d1829c1ec3c31ffb8330#diff-d8e520586ad048af79d6ff89f6b861e143f512901e02109f5201798d57923699L322' target='_blank'>Link</a></div><div id='project'> Project Name: NifTK/NiftyNet</div><div id='commit'> Commit Name: 534c4dc7423681faab78d1829c1ec3c31ffb8330</div><div id='time'> Time: </div><div id='author'> Author: null</div><div id='file'> File Name: niftynet/layer/resampler.py</div><div id='class'> Class Name: ResamplerLayer</div><div id='method'> Method Name: _resample_inv_dst_weighting</div><BR>